{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a2e07cf",
   "metadata": {},
   "source": [
    "# Tweet scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0648b529",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "#!pip install snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5161171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89de9c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#technique 1: list of words in language opposite to lang\n",
    "#TODO: estaria bé que la cerca la fes en el tuit net\n",
    "def give_me_tweets1(max_num, word_list, language):\n",
    "    text=' OR '.join(word_list)\n",
    "    tweets_list=[]\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(text+' lang:'+language).get_items()):\n",
    "        if i>=max_num:\n",
    "            break\n",
    "        tweets_list.append([tweet.date, tweet.lang, tweet.content])#tweet.id, tweet.user.username\n",
    "\n",
    "    tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Language', 'Text'])\n",
    "    return tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff89b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "give_me_tweets1(10, ['oye', 'bueno', 'pues'], 'ca')\n",
    "\n",
    "#idees per ca: vamos, bueno, hombre, venga, oye\n",
    "#idees per es: adéu, conya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd011f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#technique 2: words from different languages in same tweet\n",
    "def give_me_tweets2(max_num, word_list):\n",
    "    \n",
    "    text=' AND '.join(word_list)\n",
    "    \n",
    "    tweets_list=[]\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(text).get_items()):\n",
    "        if i>max_num:\n",
    "            break\n",
    "        tweets_list.append([tweet.date, tweet.lang, tweet.content, tweet.url])#tweet.id, tweet.user.username\n",
    "\n",
    "    tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Language', 'Text', 'URL'])\n",
    "    print(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63d0b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "give_me_tweets2(10, ['adéu', 'adiós'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6f634d",
   "metadata": {},
   "source": [
    "# Language detection tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1d568e",
   "metadata": {},
   "source": [
    "## GCLD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9db74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify catalan-spanish mixed tweets\n",
    "\n",
    "!pip install gcld3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4418f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gcld3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ea7cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = gcld3.NNetLanguageIdentifier(min_num_bytes=0, max_num_bytes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bccc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = (\"Buenos días, amigos. Cómo estáis? Estoy hablando español.\")\n",
    "results = detector.FindTopNMostFreqLangs(text=sample, num_langs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62af25be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in results:\n",
    "    print(i.language, i.is_reliable, i.proportion, i.probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f8c51c",
   "metadata": {},
   "source": [
    "## Polyglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0295a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install polyglot\n",
    "#!pip install morfessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874c11a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polyglot\n",
    "\n",
    "from polyglot.text import Text, Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017eebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = Text(\"Bonjour, Mesdames.\")\n",
    "print(\"Language Detected: Code={}, Name={}\\n\".format(text.language.code, text.language.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be82eb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from polyglot.detect import Detector\n",
    "\n",
    "mixed_text = \"Hello! How are you? Bon dia! Com esteu?\"\n",
    "\n",
    "for language in Detector(mixed_text).languages:\n",
    "        print(language)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b05f4c6",
   "metadata": {},
   "source": [
    "## Langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863754f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27677669",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from langdetect import detect, detect_langs\n",
    "result=detect_langs(\"Hola, cómo estáis, amigos? Sois tontos. Adéu, amics i amigues.\")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb060681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_mixed(text):\n",
    "    if ('es:' in str(detect_langs(text))) and ('ca:' in str(detect_langs(text))):\n",
    "        return True\n",
    "    else:\n",
    "        return False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfd60d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_mixed(\"Hola, cómo estáis, amigos? Sois tontos. Adéu, amics i amigues.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810d538d",
   "metadata": {},
   "source": [
    "## Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395b5801",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b305d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import fasttext\n",
    "PRETRAINED_MODEL_PATH = './lid.176.bin'\n",
    "model = fasttext.load_model(PRETRAINED_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14c3be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['War does not show who is right, just who is left. Hola, em dic Berta i soc de Barcelona. He estudiat filosofia i he fet un doctorat de merda.']\n",
    "predictions = model.predict(sentences)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685e3631",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13172166",
   "metadata": {},
   "source": [
    "## Langid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042b6ccb",
   "metadata": {},
   "source": [
    "## Pycld2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc4eea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pycld2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eb92fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycld2 as cld2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1689ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_en_Latn = \"\"\"\\\n",
    "France is the largest country in Western Europe. Em dic Armand, vaig nèixer a Barcelona.\"\"\"\n",
    "\n",
    "isReliable, textBytesFound, details, vectors = cld2.detect(\n",
    "    fr_en_Latn, returnVectors=True\n",
    ")\n",
    "\n",
    "print(vectors)\n",
    "print(details)\n",
    "print(textBytesFound)\n",
    "print(isReliable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e0b3a8",
   "metadata": {},
   "source": [
    "#### Conclusion thus far: Langdetect is the best tool I've tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6ed507",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I use Langdetect to filter tweets found near Barcelona\n",
    "\n",
    "tweets_list=[]\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper(\"near:BCN within:15mi\").get_items()):\n",
    "    if i>=100:\n",
    "        break\n",
    "    if len(tweet.content)>0 and is_mixed(tweet.content)==True:\n",
    "        tweets_list.append([tweet.date, tweet.content])#tweet.id, tweet.user.username\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be50fdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "print(tweets_df['Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5527a6b7",
   "metadata": {},
   "source": [
    "### I'm gonna try to clean the tweets first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4599e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install twitter-text-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa4afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttp import ttp\n",
    "\n",
    "p = ttp.Parser()\n",
    "result = p.parse(\"@burnettedmond, you now support #IvoWertzel's tweet parser! https://github.com/edmondburnett/\")\n",
    "#print(result.reply)\n",
    "#print(result.users)\n",
    "#print(result.tags)\n",
    "#print(result.urls)\n",
    "\n",
    "tweet=\"@burnettedmond, you now support #IvoWertzel's tweet parser! https://github.com/edmondburnett/\"\n",
    "clean_tweet=tweet.replace(result.reply, '')\n",
    "for item in result.users:\n",
    "    clean_tweet=tweet.replace('@'+item, '')\n",
    "#for item in result.tags:\n",
    "#    clean_tweet=clean_tweet.replace(item, '')\n",
    "for item in result.urls:\n",
    "    clean_tweet=clean_tweet.replace(item, '')\n",
    "    \n",
    "clean_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40ec7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install emoji\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236b58a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                u\"\\U00002702-\\U000027B0\"\n",
    "                u\"\\U000024C2-\\U0001F251\"\n",
    "                u\"\\U0001f926-\\U0001f937\"\n",
    "                u'\\U00010000-\\U0010ffff'\n",
    "                u\"\\u200d\"\n",
    "                u\"\\u2640-\\u2642\"\n",
    "                u\"\\u2600-\\u2B55\"\n",
    "                u\"\\u23cf\"\n",
    "                u\"\\u23e9\"\n",
    "                u\"\\u231a\"\n",
    "                u\"\\u3030\"\n",
    "                u\"\\ufe0f\"\n",
    "                        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ac783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_clean(tweet_text):\n",
    "    p = ttp.Parser()\n",
    "    result = p.parse(tweet_text)\n",
    "    for item in result.users:\n",
    "        tweet_text=tweet_text.replace('@'+item, '')\n",
    "    for item in result.urls:\n",
    "        tweet_text=tweet_text.replace(item, '')\n",
    "    tweet_text=remove_emoji(tweet_text)\n",
    "    return tweet_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692eba05",
   "metadata": {},
   "source": [
    "### Combine both functionalities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddbd748",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_list=[]\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper(\"near:BCN within:15mi\").get_items()):\n",
    "    if i>=1000:\n",
    "        break\n",
    "    tweet_text=quick_clean(tweet.content)\n",
    "    #print(i, tweet_text)\n",
    "    try:\n",
    "        if len(tweet_text)>10 and is_mixed(tweet_text)==True:\n",
    "            tweets_list.append([tweet.date, tweet_text])#tweet.id, tweet.user.username\n",
    "    except:\n",
    "        print('No language features:', tweet_text)\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18107ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eec0e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets de la juana dolores (o altres usuaris que tendeixen a fer code mixing)\n",
    "\n",
    "tweets_list=[]\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper(\"from:juanadolorex\").get_items()):\n",
    "    if i>=1000:\n",
    "        break\n",
    "    tweet_text=quick_clean(tweet.content)\n",
    "    #print(i, tweet_text)\n",
    "    try:\n",
    "        if len(tweet_text)>10 and is_mixed(tweet_text)==True:\n",
    "            tweets_list.append([tweet.date, tweet_text])#tweet.id, tweet.user.username\n",
    "    except:\n",
    "        print('No language features:', tweet_text)\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bf8a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff19d0b",
   "metadata": {},
   "source": [
    "## Tweets amb certs hashtags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e3f93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "give_me_tweets1(10, ['#lol'], 'ca') #altres: #ironia, #no. Sobretot si estan al final del tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4e125f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
