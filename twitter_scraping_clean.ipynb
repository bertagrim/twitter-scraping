{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a2e07cf",
   "metadata": {},
   "source": [
    "# Tweet scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5161171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "from ttp import ttp\n",
    "from langdetect import detect, detect_langs\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c63c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d95641",
   "metadata": {},
   "source": [
    "### Get tweets by keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3209308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_me_tweets_keyword(max_num, word_list):\n",
    "    text=' OR '.join(word_list)\n",
    "    tweets_list_ca=[]\n",
    "    tweets_list_es=[]\n",
    "    tweets_list_en=[]\n",
    "    \n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(text+' AND '+\"-FC\", \"near:BCN within:15mi\").get_items()):\n",
    "        if i>=max_num:\n",
    "            break\n",
    "        tweet_text=quick_clean(tweet.content)\n",
    "        #print(i, tweet_text)\n",
    "        try:\n",
    "            if len(tweet_text)>10:\n",
    "                if 'es:' in str(detect_langs(tweet_text)):\n",
    "                    tweets_list_es.append([tweet.date, tweet.content])\n",
    "                if ('ca:' in str(detect_langs(tweet_text))):\n",
    "                    tweets_list_ca.append([tweet.date, tweet.content])\n",
    "                if ('en:' in str(detect_langs(tweet_text))):\n",
    "                    tweets_list_en.append([tweet.date, tweet.content])\n",
    "        except:\n",
    "            print('No language features:', tweet_text)\n",
    "\n",
    "    tweets_df_ca = pd.DataFrame(tweets_list_ca, columns=['Datetime',  'Text'])\n",
    "    tweets_df_es = pd.DataFrame(tweets_list_es, columns=['Datetime',  'Text'])\n",
    "    tweets_df_en = pd.DataFrame(tweets_list_en, columns=['Datetime',  'Text'])\n",
    "    print(tweets_df_ca)\n",
    "    print(tweets_df_es)\n",
    "    print(tweets_df_en)\n",
    "    \n",
    "    \n",
    "    return tweets_df_ca, tweets_df_es, tweets_df_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a1e398",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "give_me_tweets_keyword(100, ['Barcelona'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0514a5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_me_tweets_keyword(max_num, word_list):\n",
    "    text=' OR '.join(word_list)\n",
    "    tweets_list=[]\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(text).get_items()):\n",
    "        if i>=max_num:\n",
    "            break\n",
    "        tweets_list.append([tweet.date, tweet.content])#tweet.id, tweet.user.username\n",
    "\n",
    "    tweets_df = pd.DataFrame(tweets_list, columns=['Datetime',  'Text'])\n",
    "    return tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b05f4c6",
   "metadata": {},
   "source": [
    "### Trying to detect if a tweet included words in both Catalan and Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27677669",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result=detect_langs()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb060681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_mixed(text):\n",
    "    if ('es:' in str(detect_langs(text))) and ('ca:' in str(detect_langs(text))):\n",
    "        return True\n",
    "    else:\n",
    "        return False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6ed507",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use Langdetect to filter tweets found near Barcelona\n",
    "\n",
    "tweets_list=[]\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper(\"near:BCN within:15mi\").get_items()):\n",
    "    if i>=100:\n",
    "        break\n",
    "    if len(tweet.content)>0 and is_mixed(tweet.content)==True:\n",
    "        tweets_list.append([tweet.date, tweet.content])#tweet.id, tweet.user.username\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be50fdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "print(tweets_df['Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5527a6b7",
   "metadata": {},
   "source": [
    "## Cleaning tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa4afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ttp.Parser()\n",
    "result = p.parse(\"@burnettedmond, you now support #IvoWertzel's tweet parser! https://github.com/edmondburnett/\")\n",
    "#print(result.reply)\n",
    "#print(result.users)\n",
    "#print(result.tags)\n",
    "#print(result.urls)\n",
    "\n",
    "tweet=\"@burnettedmond, you now support #IvoWertzel's tweet parser! https://github.com/edmondburnett/\"\n",
    "clean_tweet=tweet.replace(result.reply, '')\n",
    "for item in result.users:\n",
    "    clean_tweet=tweet.replace('@'+item, '')\n",
    "#for item in result.tags:\n",
    "#    clean_tweet=clean_tweet.replace(item, '')\n",
    "for item in result.urls:\n",
    "    clean_tweet=clean_tweet.replace(item, '')\n",
    "    \n",
    "clean_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c00f67",
   "metadata": {},
   "source": [
    "#### This function removes emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236b58a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                u\"\\U00002702-\\U000027B0\"\n",
    "                u\"\\U000024C2-\\U0001F251\"\n",
    "                u\"\\U0001f926-\\U0001f937\"\n",
    "                u'\\U00010000-\\U0010ffff'\n",
    "                u\"\\u200d\"\n",
    "                u\"\\u2640-\\u2642\"\n",
    "                u\"\\u2600-\\u2B55\"\n",
    "                u\"\\u23cf\"\n",
    "                u\"\\u23e9\"\n",
    "                u\"\\u231a\"\n",
    "                u\"\\u3030\"\n",
    "                u\"\\ufe0f\"\n",
    "                        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2efc4d7",
   "metadata": {},
   "source": [
    "### Final cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ac783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_clean(tweet_text):\n",
    "    p = ttp.Parser()\n",
    "    result = p.parse(tweet_text)\n",
    "    for item in result.users:\n",
    "        tweet_text=tweet_text.replace('@'+item, '')\n",
    "    for item in result.urls:\n",
    "        tweet_text=tweet_text.replace(item, '')\n",
    "    tweet_text=remove_emoji(tweet_text)\n",
    "    return tweet_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692eba05",
   "metadata": {},
   "source": [
    "### Clean the tweets and then select those that are mixed (have CA and ES in it) and that have been published near Barcelona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddbd748",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_list=[]\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper(\"near:BCN within:15mi\").get_items()):\n",
    "    if i>=1000:\n",
    "        break\n",
    "    tweet_text=quick_clean(tweet.content)\n",
    "    try:\n",
    "        if len(tweet_text)>10 and is_mixed(tweet_text)==True:\n",
    "            tweets_list.append([tweet.date, tweet_text])#tweet.id, tweet.user.username\n",
    "    except:\n",
    "        print('No language features:', tweet_text)\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef70294",
   "metadata": {},
   "source": [
    "### Search by username (and also includes the mixed language thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eec0e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_list=[]\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper(\"from:USER_NAME\").get_items()):\n",
    "    if i>=1000:\n",
    "        break\n",
    "    tweet_text=quick_clean(tweet.content)\n",
    "    try:\n",
    "        if len(tweet_text)>10 and is_mixed(tweet_text)==True:\n",
    "            tweets_list.append([tweet.date, tweet_text])#tweet.id, tweet.user.username\n",
    "    except:\n",
    "        print('No language features:', tweet_text)\n",
    "\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
